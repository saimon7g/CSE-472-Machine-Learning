{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import f1_score , confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / (input_dim + output_dim)) # ensure tthe variance\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(input_data, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    def __init__(self, input_dim, epsilon=1e-8, momentum=0.9):\n",
    "        self.gamma = np.ones((1, input_dim))\n",
    "        self.beta = np.zeros((1, input_dim))\n",
    "        self.epsilon = epsilon\n",
    "        self.running_mean = np.zeros((1, input_dim))\n",
    "        self.running_var = np.ones((1, input_dim))\n",
    "        self.momentum = 0.9\n",
    "        self.input = None\n",
    "        self.normalized = None\n",
    "        self.std = None\n",
    "        self.var = None\n",
    "        self.mean = None\n",
    "        \n",
    "    def forward(self, input_data, training=True):\n",
    "        self.input = input_data\n",
    "        \n",
    "        if training:\n",
    "            self.mean = np.mean(input_data, axis=0, keepdims=True)\n",
    "            self.var = np.var(input_data, axis=0, keepdims=True)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            self.mean = self.running_mean\n",
    "            self.var = self.running_var\n",
    "        \n",
    "        self.std = np.sqrt(self.var + self.epsilon)\n",
    "        self.normalized = (input_data - self.mean) / self.std\n",
    "        \n",
    "        return self.gamma * self.normalized + self.beta\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        batch_size = grad_output.shape[0]\n",
    "        \n",
    "        grad_gamma = np.sum(grad_output * self.normalized, axis=0, keepdims=True)\n",
    "        grad_beta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        grad_normalized = grad_output * self.gamma\n",
    "        grad_var = -0.5 * np.sum(grad_normalized * (self.input - self.mean) / (self.std ** 3), axis=0, keepdims=True)\n",
    "        grad_mean = -np.sum(grad_normalized / self.std, axis=0, keepdims=True) - 2 * grad_var * np.mean(self.input - self.mean, axis=0, keepdims=True)\n",
    "        grad_input = grad_normalized / self.std + 2 * grad_var * (self.input - self.mean) / batch_size + grad_mean / batch_size\n",
    "        \n",
    "        self.gamma -= learning_rate * grad_gamma\n",
    "        self.beta -= learning_rate * grad_beta\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.maximum(0, input_data)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input > 0)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, drop_rate=0.5):\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1-self.drop_rate, size=input_data.shape) / (1-self.drop_rate)\n",
    "            return input_data * self.mask\n",
    "        return input_data\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if not self.m:\n",
    "            for key in params:\n",
    "                self.m[key] = np.zeros_like(params[key])\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        for key in params:\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "            \n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            params[key] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        exp = np.exp(input_data - np.max(input_data, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.output * (1 - self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, learning_rate=0.001,dropout_rate=0.3):\n",
    "        self.layers = []\n",
    "        layer_sizes = [input_size] + hidden_sizes + [num_classes]\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Create the network architecture\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(Dense(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes) - 2:  # Don't add these layers after the last Dense layer\n",
    "                self.layers.append(BatchNorm(layer_sizes[i+1]))\n",
    "                self.layers.append(ReLU())\n",
    "                self.layers.append(Dropout(dropout_rate))\n",
    "        \n",
    "        self.layers.append(Softmax())\n",
    "        self.optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        # Reshape input if necessary\n",
    "        if len(x.shape) == 3:  # If input is (N, height, width)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "        # Normalize input\n",
    "        x = x.astype(np.float32) / 255.0\n",
    "        \n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (BatchNorm, Dropout)):\n",
    "                output = layer.forward(output, training)\n",
    "            else:\n",
    "                output = layer.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, (Dense, BatchNorm)):\n",
    "                grad_output = layer.backward(grad_output, self.optimizer.learning_rate)\n",
    "            else:\n",
    "                grad_output = layer.backward(grad_output)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=64):\n",
    "        # Convert labels to one-hot encoding\n",
    "        enc = OneHotEncoder(sparse_output=False)\n",
    "        y_train_oh = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_val_oh = enc.transform(y_val.reshape(-1, 1))\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        val_f1s = []\n",
    "        \n",
    "        n_samples = X_train.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_train = X_train[indices]\n",
    "            y_train_oh = y_train_oh[indices]\n",
    "            y_train = y_train[indices]  # Also shuffle original labels\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            correct_predictions = 0\n",
    "            \n",
    "            # Training\n",
    "            for batch in tqdm(range(n_batches), desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                X_batch = X_train[start_idx:end_idx]\n",
    "                y_batch_oh = y_train_oh[start_idx:end_idx]\n",
    "                y_batch = y_train[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.forward(X_batch, training=True)\n",
    "                predictions = np.argmax(output, axis=1)\n",
    "                correct_predictions += np.sum(predictions == y_batch)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = -np.mean(np.sum(y_batch_oh * np.log(output + 1e-8), axis=1))\n",
    "                epoch_loss += loss\n",
    "                \n",
    "                # Backward pass\n",
    "                grad_output = output - y_batch_oh\n",
    "                self.backward(grad_output)\n",
    "            \n",
    "            # Calculate training metrics\n",
    "            train_loss = epoch_loss / n_batches\n",
    "            train_acc = correct_predictions / n_samples\n",
    "            \n",
    "            # Validation metrics\n",
    "            val_output = self.forward(X_val, training=False)\n",
    "            val_loss = -np.mean(np.sum(y_val_oh * np.log(val_output + 1e-8), axis=1))\n",
    "            val_pred = np.argmax(val_output, axis=1)\n",
    "            val_acc = np.mean(val_pred == y_val)\n",
    "            val_f1 = f1_score(y_val, val_pred, average='macro')\n",
    "            \n",
    "            # Store metrics\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            val_f1s.append(val_f1)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "            print(f'Val Macro-F1: {val_f1:.4f}\\n')\n",
    "        \n",
    "        return train_losses, val_losses, train_accs, val_accs, val_f1s\n",
    "    \n",
    "    def get_weights(self):\n",
    "        weights = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Dense):\n",
    "                weights[f'W{i}'] = layer.weights\n",
    "                weights[f'b{i}'] = layer.bias\n",
    "        return weights\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Dense):\n",
    "                layer.weights = weights[f'W{i}']\n",
    "                layer.bias = weights[f'b{i}']\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(validation_split=0.1, random_seed=42):\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Load the data\n",
    "    transform = transforms.ToTensor()\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train = train_dataset.data.numpy()\n",
    "    y_train = train_dataset.targets.numpy()\n",
    "    X_test = test_dataset.data.numpy()\n",
    "    y_test = test_dataset.targets.numpy()\n",
    "    \n",
    "    # Reshape the data\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)  # Flatten 28x28 to 784\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)     # Flatten 28x28 to 784\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.astype(np.float32))\n",
    "    X_test = scaler.transform(X_test.astype(np.float32))\n",
    "    \n",
    "    # Create validation split\n",
    "    n_train = X_train.shape[0]\n",
    "    indices = np.random.permutation(n_train)\n",
    "    n_val = int(validation_split * n_train)\n",
    "    train_indices, val_indices = indices[n_val:], indices[:n_val]\n",
    "    \n",
    "    # Split the data\n",
    "    X_val = X_train[val_indices]\n",
    "    y_val = y_train[val_indices]\n",
    "    X_train = X_train[train_indices]\n",
    "    y_train = y_train[train_indices]\n",
    "    \n",
    "    # Print dataset information\n",
    "    print(\"Dataset shapes:\")\n",
    "    print(f\"Training set: {X_train.shape} with labels {y_train.shape}\")\n",
    "    print(f\"Validation set: {X_val.shape} with labels {y_val.shape}\")\n",
    "    print(f\"Test set: {X_test.shape} with labels {y_test.shape}\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(\"Training set:\")\n",
    "    for i in range(10):\n",
    "        count = np.sum(y_train == i)\n",
    "        percentage = count / len(y_train) * 100\n",
    "        print(f\"Class {i}: {count} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(conf_matrix, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add labels\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(conf_matrix.shape[0])]\n",
    "    \n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_per_class_metrics(per_class_acc, per_class_f1, class_names=None):\n",
    "    \"\"\"\n",
    "    Plot per-class accuracy and F1-scores.\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(len(per_class_acc))]\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    rects1 = ax.bar(x - width/2, per_class_acc, width, label='Accuracy')\n",
    "    rects2 = ax.bar(x + width/2, per_class_f1, width, label='F1-score')\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Per-class Performance')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_losses, val_losses, train_accs, val_accs, val_f1s):\n",
    "    epochs = len(train_losses)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs + 1), val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(1, epochs + 1), train_accs, label='Train Accuracy')\n",
    "    plt.plot(range(1, epochs + 1), val_accs, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    \n",
    "    # Plot F1 scores\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(range(1, epochs + 1), val_f1s, label='Validation Macro-F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Macro-F1 Score')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Macro-F1 Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Fashion MNIST class names\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    num_classes = 10 \n",
    "    input_size = 784  # 28x28 pixels\n",
    "    print(\"Loading data...\")\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = load_and_preprocess_data()\n",
    "    \n",
    "    \n",
    "    # Initialize model\n",
    "    hidden_sizes = [512, 256 , 128 ]\n",
    "    learning_rate = 0.008\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    \n",
    "    print(\"Initializing model...\")\n",
    "    model = NeuralNetwork(input_size, hidden_sizes, num_classes, learning_rate)\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    metrics = model.train(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "     # Save only the weights\n",
    "    weights = model.get_weights()\n",
    "    with open('weights.pkl', 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "    \n",
    "    # Load the weights back into a new model instance\n",
    "    new_model = NeuralNetwork(input_size, hidden_sizes, num_classes, learning_rate)\n",
    "    with open('weights.pkl', 'rb') as f:\n",
    "        loaded_weights = pickle.load(f)\n",
    "    new_model.set_weights(loaded_weights)\n",
    "    print(\"Model weights loaded into new model instance.\")\n",
    "    \n",
    "    # new_model = model\n",
    "    \n",
    "    \n",
    "    # Create encoder for test set evaluation\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    enc = OneHotEncoder(sparse_output=False)\n",
    "    enc.fit(y_train.reshape(-1, 1))  # Fit on training data\n",
    "    \n",
    "    # Test set evaluation\n",
    "    predictions = new_model.forward(x_test, training=False)\n",
    "    test_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    test_acc = np.mean(test_pred == y_test)\n",
    "    test_macro_f1 = f1_score(y_test, test_pred, average='macro')\n",
    "    test_loss = -np.mean(np.sum(enc.transform(y_test.reshape(-1, 1)) * \n",
    "                               np.log(predictions + 1e-8), axis=1))\n",
    "    \n",
    "    # Print overall results\n",
    "    print(\"\\n=== Test Set Results ===\")\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "    print(f'Test Macro-F1: {test_macro_f1:.4f}')\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    per_class_acc = []\n",
    "    per_class_f1 = f1_score(y_test, test_pred, average=None)\n",
    "    conf_matrix = confusion_matrix(y_test, test_pred)\n",
    "    \n",
    "    # Print per-class performance\n",
    "    print(\"\\nPer-class Performance:\")\n",
    "    print(\"Class\\t\\tAccuracy\\tF1-Score\")\n",
    "    print(\"-\" * 50)\n",
    "    for class_idx in range(num_classes):\n",
    "        class_mask = y_test == class_idx\n",
    "        class_acc = np.mean(test_pred[class_mask] == y_test[class_mask])\n",
    "        per_class_acc.append(class_acc)\n",
    "        print(f\"{class_names[class_idx]:<15} {class_acc:.4f}\\t{per_class_f1[class_idx]:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    print(\"\\nGenerating visualization plots...\")\n",
    "    plot_metrics(*metrics)\n",
    "    plot_confusion_matrix(conf_matrix, class_names)\n",
    "    plot_per_class_metrics(per_class_acc, per_class_f1, class_names)\n",
    "    \n",
    "    # Save results to file\n",
    "    print(\"\\nSaving results to 'model_results.txt'...\")\n",
    "    with open('model_results.txt', 'w') as f:\n",
    "        f.write(\"=== Model Configuration ===\\n\")\n",
    "        f.write(f\"Hidden layers: {hidden_sizes}\\n\")\n",
    "        f.write(f\"Learning rate: {learning_rate}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=== Test Set Results ===\\n\")\n",
    "        f.write(f'Test Loss: {test_loss:.4f}\\n')\n",
    "        f.write(f'Test Accuracy: {test_acc:.4f}\\n')\n",
    "        f.write(f'Test Macro-F1: {test_macro_f1:.4f}\\n\\n')\n",
    "        \n",
    "        f.write(\"=== Per-class Performance ===\\n\")\n",
    "        for i in range(num_classes):\n",
    "            f.write(f\"{class_names[i]:<15} Acc: {per_class_acc[i]:.4f} F1: {per_class_f1[i]:.4f}\\n\")\n",
    "    \n",
    "    print(\"\\nEvaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
