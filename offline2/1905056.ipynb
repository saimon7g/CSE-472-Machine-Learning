{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import all libraries\n",
    "create class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python class for normal logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, weights=None, bias=None, learningRate=0.01, epochs=1000,selectedFeatures=None):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.learningRate = learningRate\n",
    "        self.epochs = epochs\n",
    "        self.selectedFeatures = selectedFeatures\n",
    "    \n",
    "\n",
    "    def loss (self, y, y_hat):\n",
    "        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        numOfSamples = X.shape[0]\n",
    "        numOfFeatures = X.shape[1]\n",
    "        weights = np.zeros(numOfFeatures)\n",
    "        bias = 0\n",
    "        learningRate = 0.01\n",
    "        epochs = 1000\n",
    "        selectedFeatures = np.random.choice(numOfFeatures, 2, replace=False)\n",
    "        print(selectedFeatures)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            y_hat = self.__sigmoid(np.dot(X, weights) + bias)\n",
    "            dw = (1 / numOfSamples) * np.dot(X.T, (y_hat - y))\n",
    "            db = (1 / numOfSamples) * np.sum(y_hat - y)\n",
    "            weights -= learningRate * dw\n",
    "            bias -= learningRate * db\n",
    "\n",
    "            # if i % 100 == 0:\n",
    "                # print(f'Epoch {i} loss: {self.loss(y, y_hat)}')\n",
    "\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.selectedFeatures = selectedFeatures\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = self.__sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        # make sure the output is binary\n",
    "        y_hat = np.round(y_hat)\n",
    "        return y_hat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreProcessing_creditcard():\n",
    "    df = pd.read_csv('../Dataset/creditcard.csv')\n",
    "    numOfAttribuetes = df.shape[1]\n",
    "    numOfRecords = df.shape[0]\n",
    "    # print(f'Number of records: {numOfRecords}')\n",
    "    # print(f'Number of attributes: {numOfAttribuetes}')\n",
    "    # print(df.columns)\n",
    "    # print(df.head(10))\n",
    "    # print(df.describe())\n",
    "    # print(df.info())\n",
    "\n",
    "    #1 Checking for missing values  -----> use one less sum to see detailed missing values\n",
    "    missingValues = df.isnull().sum().sum()\n",
    "    # print(f'Missing values: {missingValues}')\n",
    "    #2 Checking for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    # print(f'Duplicates: {duplicates}')\n",
    "    # duplicateRows = df[df.duplicated()]\n",
    "    # print(f'Duplicate rows: {duplicateRows}')\n",
    "\n",
    "    #3 replace missing values with mean ( numerical attributes)\n",
    "    df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "    #4 remove exact duplicates rows, keep the first one\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # print(f'Number of records after removing duplicates: {df.shape[0]}')\n",
    "\n",
    "    #5 if any target value is missing, remove the record\n",
    "    df.dropna(subset=['Class'], inplace=True)\n",
    "    print(f'Number of records after removing missing target values: {df.shape[0]}')\n",
    "\n",
    "    #6 split the data into features and target\n",
    "    target_column = 'Class' \n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    # print(X.shape)\n",
    "    # print(y.shape)\n",
    "    # print(y.value_counts())\n",
    "    # print(X.columns)\n",
    "\n",
    "    #7 display non numerical attributes\n",
    "    nonNumericalAttributes = X.select_dtypes(include=['object']).columns\n",
    "    # print(f'Non numerical attributes: {nonNumericalAttributes}')\n",
    "\n",
    "    # 8 scale the features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = scaler.fit_transform(X)\n",
    "\n",
    "    #9 make x a dataframe again\n",
    "    X = pd.DataFrame(scaled_X, columns=X.columns)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreProcessing_iris():\n",
    "    df = pd.read_csv('../Dataset/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    numOfAttribuetes = df.shape[1]\n",
    "    numOfRecords = df.shape[0]\n",
    "    print(f'Number of records: {numOfRecords}')\n",
    "    print(f'Number of attributes: {numOfAttribuetes}')\n",
    "    print(df.columns)\n",
    "    print(df.head(10))\n",
    "    print(df.describe())\n",
    "    print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(X, y, test_size=0.2):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "def bagging_samples(X_train, y_train):\n",
    "    # set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    # number of samples (rows) in the training set\n",
    "    n_samples = X_train.shape[0]\n",
    "    \n",
    "    # generate 9 bagging samples\n",
    "    samples = [resample(X_train, y_train, n_samples=n_samples, replace=True) for _ in range(9)]\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_stacking(data_sets, X_val, y_val):\n",
    "\n",
    "     # 2 - Train 9 different model with 9 different samples\n",
    "    models = []\n",
    "    for i in range(9):\n",
    "        model = LogisticRegression()\n",
    "        model.fit(data_sets[i][0], data_sets[i][1])\n",
    "        models.append(model)\n",
    "\n",
    "    # 3 -make predictions on the validation set\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_preds.append(model.predict(X_val))\n",
    "    y_preds = np.array(y_preds)\n",
    "\n",
    "    # 4 build a meta model usiiing the predictions from the 9 models\n",
    "    meta_model = LogisticRegression()\n",
    "    meta_model.fit(y_preds.T, y_val)\n",
    "\n",
    "    return models, meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackig_predict(models, meta_model, X_test):\n",
    "    y_preds_test = []\n",
    "    for model in models:\n",
    "        y_preds_test.append(model.predict(X_test))\n",
    "    y_preds_test = np.array(y_preds_test)\n",
    "\n",
    "    y_pred_meta = meta_model.predict(y_preds_test.T)\n",
    "    return y_pred_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(models, X_test):\n",
    "\n",
    "    y_preds_test = []\n",
    "    for model in models:\n",
    "        y_preds_test.append(model.predict(X_test))\n",
    "    y_preds_test = np.array(y_preds_test)\n",
    "\n",
    "    y_pred_majority = np.round(np.mean(y_preds_test, axis=0))\n",
    "    return y_pred_majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_star(models, X_test):\n",
    "# For LR, report average Â± stdev for the 9 bagging LR learners\n",
    "    y_preds_test = []\n",
    "    for model in models:\n",
    "        y_preds_test.append(model.predict(X_test))\n",
    "    y_preds_test = np.array(y_preds_test)\n",
    "    \n",
    "    y_pred = np.mean(y_preds_test, axis=0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_pred):\n",
    "# Accuracy Sensitivity Specificity Precision F1-score AUROC AUPR\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    aupr = average_precision_score(y_test, y_pred)\n",
    "\n",
    "    # return as a dictionary\n",
    "    return {'accuracy': accuracy, 'sensitivity': sensitivity, 'specificity': specificity, 'precision': precision, 'f1': f1, 'auroc': auroc, 'aupr': aupr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main Function, make changes here to slect different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSetLocation:\n",
    "\n",
    "../Dataset/creditcard.csv\n",
    "../WA_Fn-UseC_-Telco-Customer-Churn.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after removing missing target values: 283726\n",
      "[14  9]\n",
      "[4 8]\n",
      "[26  3]\n",
      "[15 14]\n",
      "[6 8]\n",
      "[18  3]\n",
      "[23  4]\n",
      "[12  3]\n",
      "[10  0]\n",
      "[2 5]\n",
      "Meta model metrics: {'accuracy': 0.998413985126705, 'sensitivity': np.float64(0.0), 'specificity': np.float64(1.0), 'precision': np.float64(0.0), 'f1': np.float64(0.0), 'auroc': np.float64(0.5), 'aupr': np.float64(0.0015860148732950341)}\n",
      "Voting model metrics: {'accuracy': 0.9989602791386177, 'sensitivity': np.float64(0.4), 'specificity': np.float64(0.9999117480937588), 'precision': np.float64(0.8780487804878049), 'f1': np.float64(0.549618320610687), 'auroc': np.float64(0.6999558740468794), 'aupr': np.float64(0.352171121119099)}\n",
      "LR_star model metrics: {'accuracy': 0.9989602791386177, 'sensitivity': np.float64(0.4), 'specificity': np.float64(0.9999117480937588), 'precision': np.float64(0.8780487804878049), 'f1': np.float64(0.549618320610687), 'auroc': np.float64(0.6999558740468794), 'aupr': np.float64(0.352171121119099)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saimon7/Documents/CSE-472-Machine-Learning/env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "# 1 - Data Preprocessing for creditcard dataset\n",
    "    X,y = dataPreProcessing_creditcard()\n",
    "    X_train, X_test, y_train, y_test = test_train_split(X, y, test_size=0.2)\n",
    "    X_train, X_val, y_train, y_val = test_train_split(X_train, y_train, test_size=0.2)\n",
    "    data_sets = bagging_samples(X_train, y_train)\n",
    "\n",
    "# 2  - Bagging and Stacking ensemble model preparation\n",
    "    base_model, meta_model = bagging_stacking(data_sets, X_val, y_val)\n",
    "# 3 - Predict the test set for stacking_ensemble\n",
    "    y_pred_meta = stackig_predict(base_model, meta_model, X_test)\n",
    "    y_pred_voting = majority_voting(base_model, X_test)\n",
    "    y_pred_LR_star= LR_star(base_model, X_test)\n",
    "\n",
    "    # make sure the output is binary\n",
    "    y_pred_meta = np.round(y_pred_meta)\n",
    "    y_pred_voting = np.round(y_pred_voting)\n",
    "    y_pred_LR_star = np.round(y_pred_LR_star)\n",
    "    \n",
    "# 4 - Calculate matrics of the models\n",
    "    metrics_meta = calculate_metrics(y_test, y_pred_meta)\n",
    "    metrics_voting = calculate_metrics(y_test, y_pred_voting)\n",
    "    metrics_LR_star = calculate_metrics(y_test, y_pred_LR_star)\n",
    "\n",
    "    print(f'Meta model metrics: {metrics_meta}')\n",
    "    print(f'Voting model metrics: {metrics_voting}')\n",
    "    print(f'LR_star model metrics: {metrics_LR_star}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
